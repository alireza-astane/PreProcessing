{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for Categorical Data (Nominal & Ordinal) in Machine Learning\n",
    "#### Categorical data needs to be transformed into a numerical format for machine learning models to understand and process it. There are two main types of categorical data:\n",
    "\n",
    "#### Nominal Data â€“ Categories have no inherent order (e.g., color: red, blue, green).\n",
    "#### Ordinal Data â€“ Categories have a meaningful order (e.g., education level: High School < Bachelor's < Master's < PhD).\n",
    "#### Let's explore different preprocessing techniques for both nominal and ordinal data, along with Python examples and a comparison of methods.\n",
    "#### \n",
    "#### 1. Encoding Nominal Data (No Order)\n",
    "#### For nominal categorical data, we use one-hot encoding or label encoding.\n",
    "#### \n",
    "#### 1.1 One-Hot Encoding (OHE)\n",
    "#### One-hot encoding converts categorical variables into multiple binary columns, each representing a category.\n",
    "#### \n",
    "#### Example: One-Hot Encoding in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color_Blue  Color_Green  Color_Red\n",
      "0       False        False       True\n",
      "1        True        False      False\n",
      "2       False         True      False\n",
      "3        True        False      False\n",
      "4       False        False       True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']})\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df, columns=['Color'])\n",
    "\n",
    "print(df_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ“Œ When to use?\n",
    "#### \n",
    "#### When categories do not have an order.\n",
    "#### Suitable for tree-based models (e.g., random forests, XGBoost).\n",
    "#### Not efficient when there are many unique categories (high-dimensionality issue).\n",
    "#### 1.2 Label Encoding\n",
    "#### Label encoding assigns an integer value to each category.\n",
    "#### \n",
    "#### Example: Label Encoding in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color  Color_Encoded\n",
      "0    Red              2\n",
      "1   Blue              0\n",
      "2  Green              1\n",
      "3   Blue              0\n",
      "4    Red              2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample dataset\n",
    "df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']})\n",
    "\n",
    "# Apply Label Encoding\n",
    "encoder = LabelEncoder()\n",
    "df['Color_Encoded'] = encoder.fit_transform(df['Color'])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ“Œ When to use?\n",
    "#### \n",
    "#### When categories do not have an order but you need a compact representation.\n",
    "#### Works well for tree-based models but can introduce bias in linear models.\n",
    "#### Not recommended for ordinal data.\n",
    "#### 2. Encoding Ordinal Data (Ordered Categories)\n",
    "#### Ordinal categorical data has a meaningful order, so simple integer mapping or ordinal encoding is preferred.\n",
    "#### \n",
    "#### 2.1 Ordinal Encoding (Manual Mapping)\n",
    "#### This method assigns meaningful integer values based on the order of categories.\n",
    "#### \n",
    "#### Example: Ordinal Encoding in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Education  Education_Encoded\n",
      "0  High School                  1\n",
      "1     Bachelor                  2\n",
      "2       Master                  3\n",
      "3          PhD                  4\n",
      "4     Bachelor                  2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample dataset\n",
    "df = pd.DataFrame({'Education': ['High School', 'Bachelor', 'Master', 'PhD', 'Bachelor']})\n",
    "\n",
    "# Define the mapping\n",
    "education_mapping = {'High School': 1, 'Bachelor': 2, 'Master': 3, 'PhD': 4}\n",
    "\n",
    "# Apply mapping\n",
    "df['Education_Encoded'] = df['Education'].map(education_mapping)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ“Œ When to use?\n",
    "#### \n",
    "#### When categories have a clear ranking (e.g., education level, satisfaction score).\n",
    "#### Works well for linear models that assume numerical order.\n",
    "#### 2.2 Ordinal Encoding using Sklearn\n",
    "#### Instead of manual mapping, we can use OrdinalEncoder from sklearn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Education  Education_Encoded\n",
      "0  High School                0.0\n",
      "1     Bachelor                1.0\n",
      "2       Master                2.0\n",
      "3          PhD                3.0\n",
      "4     Bachelor                1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Sample dataset\n",
    "df = pd.DataFrame({'Education': ['High School', 'Bachelor', 'Master', 'PhD', 'Bachelor']})\n",
    "\n",
    "# Apply Ordinal Encoding\n",
    "encoder = OrdinalEncoder(categories=[['High School', 'Bachelor', 'Master', 'PhD']])\n",
    "df['Education_Encoded'] = encoder.fit_transform(df[['Education']])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ“Œ When to use?\n",
    "#### \n",
    "#### When there is an inherent order in the data.\n",
    "#### Recommended when using linear regression, logistic regression.\n",
    "#### 3. Frequency Encoding (For High Cardinality Categories)\n",
    "#### This method replaces categories with their frequency of occurrence.\n",
    "\n",
    "#### Example: Frequency Encoding in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  City  City_Encoded\n",
      "0   NY      0.333333\n",
      "1   LA      0.222222\n",
      "2   SF      0.444444\n",
      "3   NY      0.333333\n",
      "4   LA      0.222222\n",
      "5   SF      0.444444\n",
      "6   NY      0.333333\n",
      "7   SF      0.444444\n",
      "8   SF      0.444444\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample dataset\n",
    "df = pd.DataFrame({'City': ['NY', 'LA', 'SF', 'NY', 'LA', 'SF', 'NY', 'SF', 'SF']})\n",
    "\n",
    "# Compute frequency\n",
    "freq_encoding = df['City'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "# Apply encoding\n",
    "df['City_Encoded'] = df['City'].map(freq_encoding)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ“Œ When to use?\n",
    "#### \n",
    "#### When categorical data has many unique categories (e.g., city names, products).\n",
    "#### Works well with tree-based models, but may cause information leakage in test data.\n",
    "#### 4. Target Encoding (For High Cardinality Categories in Regression)\n",
    "#### This replaces categories with the mean of the target variable.\n",
    "#### \n",
    "#### Example: Target Encoding in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  City  Price  City_Encoded\n",
      "0   NY    100         110.0\n",
      "1   LA    200         205.0\n",
      "2   SF    150         160.0\n",
      "3   NY    120         110.0\n",
      "4   LA    210         205.0\n",
      "5   SF    170         160.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample dataset\n",
    "df = pd.DataFrame({'City': ['NY', 'LA', 'SF', 'NY', 'LA', 'SF'],\n",
    "                   'Price': [100, 200, 150, 120, 210, 170]})\n",
    "\n",
    "# Compute target mean encoding\n",
    "target_mean = df.groupby('City')['Price'].mean().to_dict()\n",
    "\n",
    "# Apply encoding\n",
    "df['City_Encoded'] = df['City'].map(target_mean)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### ðŸ“Œ When to use?\n",
    "#### \n",
    "#### Useful for regression problems with high-cardinality categorical data.\n",
    "#### Can cause data leakage, so it requires k-fold cross-validation.\n",
    "#### Comparison of Categorical Encoding Methods\n",
    "#### Encoding Method\tType of Data\tPros\tCons\n",
    "#### One-Hot Encoding\tNominal\tNo assumption about order, good for tree-based models\tCreates many columns for high-cardinality data\n",
    "#### Label Encoding\tNominal\tSimple and efficient\tCan mislead linear models by introducing artificial order\n",
    "#### Ordinal Encoding\tOrdinal\tMaintains order of categories\tShould not be used for unordered data\n",
    "#### Frequency Encoding\tNominal\tReduces dimensionality, useful for high-cardinality data\tMay lose categorical information\n",
    "#### Target Encoding\tNominal\tCaptures relationship with target variable\tRisk of overfitting and data leakage\n",
    "#### Final Thoughts\n",
    "#### For small categorical variables, one-hot encoding is usually the best choice.\n",
    "#### For ordered categorical variables, ordinal encoding is appropriate.\n",
    "#### For high-cardinality data, frequency encoding or target encoding are useful.\n",
    "#### Label encoding should be avoided for nominal data unless used in tree-based models.\n",
    "#### Would you like to see specific examples with a dataset of your choice? ðŸš€\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Representation for Categorical Data\n",
    "#### Embedding representation is an advanced encoding technique used when categorical variables have high cardinality (many unique values). Instead of one-hot encoding or label encoding, which may be inefficient, we map categorical values into a dense, lower-dimensional vector space. This is common in deep learning models, especially with PyTorch and TensorFlow.\n",
    "#### \n",
    "#### Why Use Embeddings?\n",
    "#### Efficient for high-cardinality categorical data (e.g., thousands of city names, product IDs).\n",
    "#### Encodes relationships between categories in a continuous vector space.\n",
    "#### Reduces dimensionality while retaining useful information.\n",
    "#### Example: Using Embeddings with PyTorch\n",
    "#### Let's build an example where we embed a categorical feature (City) using PyTorch's nn.Embedding layer.\n",
    "#### \n",
    "#### Step 1: Prepare the Data\n",
    "#### We create a dataset with a categorical variable (City) and convert it to integer indices.\n",
    "#### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City-to-Index Mapping: {'Chicago': 0, 'Los Angeles': 1, 'New York': 2, 'San Francisco': 3, 'Houston': 4}\n",
      "City Indices: tensor([2, 1, 3, 0, 4, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Sample categorical data\n",
    "cities = [\"New York\", \"Los Angeles\", \"San Francisco\", \"Chicago\", \"Houston\", \"New York\", \"Chicago\"]\n",
    "\n",
    "# Create a mapping of categories to indices\n",
    "city_to_idx = {city: idx for idx, city in enumerate(set(cities))}\n",
    "city_indices = torch.tensor([city_to_idx[city] for city in cities])\n",
    "\n",
    "print(\"City-to-Index Mapping:\", city_to_idx)\n",
    "print(\"City Indices:\", city_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### City-to-Index Mapping: {'New York': 0, 'Los Angeles': 1, 'San Francisco': 2, 'Chicago': 3, 'Houston': 4}\n",
    "#### City Indices: tensor([0, 1, 2, 3, 4, 0, 3])\n",
    "#### Step 2: Define an Embedding Layer\n",
    "#### The embedding layer maps each categorical value (index) into a dense embedding vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Representation:\n",
      " tensor([[-0.8344, -0.4868, -0.9831],\n",
      "        [ 1.4175, -2.2451,  0.5136],\n",
      "        [ 0.6970, -0.3020,  1.1054],\n",
      "        [-0.9581, -0.7232, -0.0923],\n",
      "        [ 0.8157,  0.4137,  0.1264],\n",
      "        [-0.8344, -0.4868, -0.9831],\n",
      "        [-0.9581, -0.7232, -0.0923]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define embedding layer\n",
    "num_categories = len(city_to_idx)  # Number of unique categories\n",
    "embedding_dim = 3  # Size of embedding vector (hyperparameter)\n",
    "\n",
    "embedding_layer = nn.Embedding(num_categories, embedding_dim)\n",
    "\n",
    "# Apply embedding lookup\n",
    "embedded_cities = embedding_layer(city_indices)\n",
    "\n",
    "print(\"Embedded Representation:\\n\", embedded_cities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ’¡ Explanation:\n",
    "#### \n",
    "#### Each city gets a 3-dimensional vector representation instead of a one-hot encoding.\n",
    "#### The values are trainable parameters that can be learned through backpropagation.\n",
    "#### The embeddings capture semantic relationships between categories.\n",
    "#### Step 3: Training an Embedding Layer\n",
    "#### In practice, embeddings are learned during model training. Below is a simple PyTorch model that uses embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output:\n",
      " tensor([[-0.7093],\n",
      "        [ 0.1282],\n",
      "        [ 0.1894],\n",
      "        [-0.1549],\n",
      "        [-0.3064]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class CityModel(nn.Module):\n",
    "    def __init__(self, num_categories, embedding_dim):\n",
    "        super(CityModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_categories, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, 1)  # Simple linear layer for prediction\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Define model\n",
    "model = CityModel(num_categories, embedding_dim)\n",
    "\n",
    "# Example input (city indices)\n",
    "city_indices = torch.tensor([0, 1, 2, 3, 4])\n",
    "\n",
    "# Forward pass\n",
    "output = model(city_indices)\n",
    "print(\"Model Output:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ðŸ”¹ In practice, embeddings are learned when training a neural network on tasks like recommendation systems, NLP, or categorical regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
