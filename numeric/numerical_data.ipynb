{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Normalization (Min-Max Scaling)\n",
    "Normalization rescales the data to a fixed range, usually [0,1] or [-1,1]. It is particularly useful when you want to ensure that all features have the same scale, which is important for algorithms like k-nearest neighbors (kNN) or neural networks.\n",
    "\n",
    "\n",
    "(x - x_min) / (x_max - x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Data:\n",
      "[[0.  0.  0. ]\n",
      " [0.5 0.5 0.5]\n",
      " [1.  1.  1. ]]\n",
      "[[0.  0.  0. ]\n",
      " [0.5 0.5 0.5]\n",
      " [1.  1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2, 3],\n",
    "                 [4, 5, 6],\n",
    "                 [7, 8, 9]])\n",
    "\n",
    "# Normalize using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "def scaler(x):\n",
    "    return (x - x.min(axis = 0))/ (x.max(axis = 0) - x.min(axis = 0))\n",
    "\n",
    "print(\"Normalized Data:\")\n",
    "print(normalized_data)\n",
    "print(scaler(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to use:\n",
    "\n",
    "Suitable for models that rely on distance calculations (e.g., kNN, SVM).\n",
    "When features have different units (e.g., height in cm and weight in kg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization (Z-score Scaling)\n",
    "#### Standardization rescales the data to have a mean of 0 and a standard deviation of 1. It is useful when the data follows a Gaussian distribution or when distance-based models like kNN or SVM are used.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$X_{𝑠𝑡𝑎𝑛𝑑𝑎𝑟𝑑𝑖𝑧𝑒𝑑} = \\frac{X − 𝜇}{𝜎} $\n",
    "\n",
    "​\n",
    " \n",
    "#### Where:\n",
    "#### μ is the mean of the data.\n",
    "#### σ is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Data:\n",
      "[[-1.22474487 -1.22474487 -1.22474487]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 1.22474487  1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2, 3],\n",
    "                 [4, 5, 6],\n",
    "                 [7, 8, 9]])\n",
    "\n",
    "# Standardize using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Standardized Data:\")\n",
    "print(standardized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use:\n",
    "\n",
    "##### Suitable for algorithms like linear regression, logistic regression, SVM, PCA, where data assumes normal distribution.\n",
    "##### Particularly useful when your model assumes features are centered around zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Outliers\n",
    "#### Outliers are extreme values that can distort the training of many models. Handling outliers is critical to improve model robustness and prevent biased learning.\n",
    "\n",
    "#### Methods:\n",
    "\n",
    "#### Truncation (Winsorization): Cap the extreme values to a specific percentile or threshold.\n",
    "#### IQR Method: Use the interquartile range (IQR) to identify and remove outliers.\n",
    "#### Example (IQR Method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data (IQR):\n",
      "   value\n",
      "0     10\n",
      "1     20\n",
      "2     30\n",
      "4     50\n",
      "5     60\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data with outliers\n",
    "data = pd.DataFrame({'value': [10, 20, 30, 1000, 50, 60]})\n",
    "\n",
    "# Calculate IQR\n",
    "Q1 = data['value'].quantile(0.25)\n",
    "Q3 = data['value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier boundaries\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Remove outliers\n",
    "filtered_data = data[(data['value'] >= lower_bound) & (data['value'] <= upper_bound)]\n",
    "\n",
    "print(\"Filtered Data (IQR):\")\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use:\n",
    "\n",
    "#### Outliers should be handled if they are errors or represent noise. For example, if you’re predicting house prices and some prices are far outside the expected range.\n",
    "#### Different algorithms handle outliers differently, so consider the model you're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imputation\n",
    "Imputation is used when there are missing values in the dataset. Several methods exist to replace missing values:\n",
    "\n",
    "Mean/Median Imputation: Replacing missing values with the mean (for normal distribution) or median (for skewed distribution).\n",
    "KNN Imputation: Imputing missing values by using the k-nearest neighbors algorithm.\n",
    "Predictive Imputation: Using regression or other models to predict the missing values.\n",
    "Example (Mean Imputation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer,KNNImputer\n",
    "\n",
    "# Sample data with missing values\n",
    "data = np.array([[1, 2, np.nan],\n",
    "                 [4, np.nan, 6],\n",
    "                 [7, 8, 9]],dtype = np.int64)\n",
    "\n",
    "# Impute missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "print(\"Data after Imputation:\")\n",
    "print(imputed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "\n",
    "# Sample data with missing values\n",
    "data = np.array([[1, 2, np.nan],\n",
    "                 [4, np.nan, 6],\n",
    "                 [7, 8, 9]])\n",
    "\n",
    "# Impute missing values using KNN (k=2)\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "print(\"Data after KNN Imputation:\")\n",
    "print(imputed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  When to use:\n",
    "\n",
    "#### Imputation is used when missing values are not randomly distributed or are due to human error.\n",
    "#### If the missing values represent a small portion of the data, imputation can prevent significant data loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncation (Winsorization)\n",
    "#### Truncation is a technique used to cap extreme values at a specified threshold. A common form of truncation is Winsorization, where extreme values are replaced with the nearest non-extreme value. This is useful for handling outliers without completely removing them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Sample data with outliers\n",
    "data = np.array([10, 20, 30, 1000, 50, 60, 2000, 80])\n",
    "\n",
    "# Apply Winsorization (limits at 5% lower and upper percentile)\n",
    "winsorized_data = winsorize(data, limits=[0.05, 0.05])\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Winsorized Data:\", winsorized_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use:\n",
    "\n",
    "#### When outliers are present but should not be completely removed.\n",
    "#### For models that are sensitive to extreme values (e.g., linear regression, SVM).\n",
    "#### In financial data where extreme values may be errors or rare but valid cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Integration\n",
    "#### Data integration is the process of combining data from multiple sources into a single unified dataset. This is necessary in data warehousing, big data analytics, and ETL (Extract, Transform, Load) pipelines.\n",
    "\n",
    "### Types of Data Integration:\n",
    "#### Schema Integration: Combining different database schemas.\n",
    "#### Entity Resolution (Deduplication): Identifying and merging records referring to the same entity.\n",
    "#### Data Fusion: Resolving conflicts in integrated data (e.g., two datasets report different prices for the same product).\n",
    "#### Example: Merging Datasets in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Two datasets with common key 'ID'\n",
    "df1 = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35]})\n",
    "df2 = pd.DataFrame({'ID': [1, 2, 3], 'Salary': [50000, 60000, 70000]})\n",
    "\n",
    "# Merge based on 'ID'\n",
    "merged_df = pd.merge(df1, df2, on='ID')\n",
    "\n",
    "print(\"Merged DataFrame:\")\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use:\n",
    "\n",
    "#### When consolidating data from multiple sources (e.g., databases, APIs).\n",
    "#### For creating comprehensive datasets for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "#### Data transformation involves converting data into a suitable format for analysis. It includes scaling, encoding, and feature engineering.\n",
    "\n",
    "### Types of Data Transformation:\n",
    "#### Normalization & Standardization (already covered above).\n",
    "#### Log Transformation (to reduce skewness in data).\n",
    "#### Encoding (converting categorical data into numerical form).\n",
    "#### Feature Engineering (creating new meaningful features).\n",
    "#### Example: Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data with skewed distribution\n",
    "data = pd.DataFrame({'Value': [10, 50, 100, 500, 1000, 5000]})\n",
    "\n",
    "# Apply log transformation\n",
    "data['Log_Value'] = np.log1p(data['Value'])\n",
    "\n",
    "print(\"Data after Log Transformation:\")\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use:\n",
    "\n",
    "#### When data is highly skewed (e.g., income distribution).\n",
    "#### To improve performance of linear models that assume normally distributed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reduction\n",
    "#### Data reduction is used to simplify datasets while retaining important information. This is critical for big data applications where computational efficiency is important.\n",
    "\n",
    "### Methods of Data Reduction:\n",
    "#### Feature Selection – Removing irrelevant features.\n",
    "#### Principal Component Analysis (PCA) – Reducing dimensionality by finding principal components.\n",
    "#### Sampling – Using a representative subset of the data.\n",
    "#### Aggregation – Grouping data to reduce granularity (e.g., monthly instead of daily sales).\n",
    "#### Example: PCA for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Sample data (5 samples, 3 features)\n",
    "data = np.array([[2.5, 2.4, 1.2],\n",
    "                 [0.5, 0.7, 0.8],\n",
    "                 [2.2, 2.9, 1.5],\n",
    "                 [1.9, 2.2, 1.3],\n",
    "                 [3.1, 3.0, 1.7]])\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(data)\n",
    "\n",
    "print(\"Reduced Data:\")\n",
    "print(reduced_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use:\n",
    "\n",
    "#### When dealing with high-dimensional datasets (e.g., image processing, text data).\n",
    "#### To improve computational efficiency and avoid the curse of dimensionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
